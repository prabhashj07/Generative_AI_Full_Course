{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e67195-be41-448c-ae6a-64946f9a5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d8d3c-b545-4b1c-bd72-ba2611581305",
   "metadata": {},
   "source": [
    "### Tensors \n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6201cb-53b9-43aa-bd0c-e2ee8082e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd81d25d-28d4-4563-9477-0a7b67d00e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Directly from data\n",
    "x = [[1,2],[3,4]]\n",
    "x_tensor = torch.tensor(x)\n",
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8181bccd-82a1-43e7-8fdb-a07b4fe74d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# From np array \n",
    "x_array = np.array(x) \n",
    "x_array_tensor = torch.tensor(x_array)\n",
    "print(x_array_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01a39b51-8b23-4dfa-ad71-90219b8ab017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.9927, 0.5748],\n",
      "        [0.4725, 0.1033]])\n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# from another tensor \n",
    "x_ones = torch.ones_like(x_tensor) \n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_tensor, dtype=torch.float) # overrides the datatype of x\n",
    "print(x_rand)\n",
    "\n",
    "x_zeroes = torch.zeros_like(x_tensor)\n",
    "print(x_zeroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0f60e7a-c823-4b1d-830a-1c09de362b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6506, 0.6047, 0.0489],\n",
      "        [0.2113, 0.1906, 0.3175]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# with random or constant value \n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4aeb43-8dd5-4a84-a2df-edc7058db04a",
   "metadata": {},
   "source": [
    "In Python, adding a trailing comma after the last item in a tuple is a matter of style and doesn't affect the functionality. However, in certain contexts, it can improve code readability and make it easier to maintain, especially when modifying the tuple by adding or removing elements.\n",
    "\n",
    "The version with the trailing comma `(2, 3,)` can be considered a good practice in some coding styles or guidelines because it makes it clear that the tuple has more than one element, even if there's only one element present. This can prevent errors when adding more elements to the tuple in the future, as you won't need to remember to add a comma after the last element.\n",
    "\n",
    "In summary, while it's not strictly necessary, adding a trailing comma after the last item in a tuple can be considered a good practice for consistency and readability in Python code. However, whether or not to use it ultimately depends on the coding style guide you or your team follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5ef57-6510-4312-83fd-756d74bf4a04",
   "metadata": {},
   "source": [
    "## Tensor Attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66cc7653-0edb-4a7a-911d-caded5380b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf89c1-bdb9-445b-aefe-12ce2e10750f",
   "metadata": {},
   "source": [
    "### Basic autograds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c1ea1-f1e7-4dfc-8e82-86766481a635",
   "metadata": {},
   "source": [
    "Gradients are quite important for the optimization purpose. PyTorch provides package which can do all the computation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf79ba9c-db0c-4bf8-b16a-3c7d77753db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2541,  2.1801, -1.8786])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068321a3-7414-4690-b26e-c97af961f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3224, -0.5739, -0.6564], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True) # by default requires_grad = false \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1038999-4d25-4182-a2bd-83672d5ba762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3224, 1.4261, 1.3436], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2 # will create the computational graph for us \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd66dde-d223-4adf-a4c0-416f3676f204",
   "metadata": {},
   "source": [
    "Here, AddBackward can be seen as we have done the backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b82340-360c-49d9-8294-29d62a691c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.7874,  4.0678,  3.6103], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*2 \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05656eef-e31f-4eda-b255-881b0f41c71c",
   "metadata": {},
   "source": [
    "Here, MulBackward can be seen as we are doing multiplication operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260539a8-af1f-42d4-b2bb-7ff5380dc823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1552, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = z.mean()\n",
    "print(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd10f75-158d-4d5a-8a6f-bfb7636572c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0966, 1.9015, 1.7914])\n"
     ]
    }
   ],
   "source": [
    "# to calculate the gradient \n",
    "z.backward() # dz/dx\n",
    "print(x.grad) # x will store the gradient value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d54cce5-11b5-4dd0-a9ef-384a914e9eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.1761, 3.6343, 1.1194], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad can only be implicitly created only for scalar outputs\n",
    "x = torch.randn(3, requires_grad = True ) \n",
    "y = x + 2 \n",
    "\n",
    "z = y*y*2 \n",
    "# z.mean()\n",
    "print(z) # z is not a scalar value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc1e19a2-3789-41f1-9ffc-8e5910970a22",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# to calculate gradient \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    252\u001b[0m     (inputs,)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# to calculate gradient \n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ed0062b-c569-4a40-ae08-a803558b3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.0291e-01, 5.3921e+00, 2.9926e-03])\n"
     ]
    }
   ],
   "source": [
    "# solution is multipying it with an vector \n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float32)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbc9ef-c35a-41d7-b2ea-81f3232bfea3",
   "metadata": {},
   "source": [
    "##### Preventing Gradient History\n",
    "There are three ways to do this : \n",
    "1. x_requires_grad_(False)\n",
    "2. x.detach()\n",
    "3. with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6d673-df33-41f1-947d-b6650da6837f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a1244-ef6e-4492-8698-f0fd5884a56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
