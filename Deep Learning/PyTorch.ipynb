{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e67195-be41-448c-ae6a-64946f9a5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d8d3c-b545-4b1c-bd72-ba2611581305",
   "metadata": {},
   "source": [
    "### Tensors \n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6201cb-53b9-43aa-bd0c-e2ee8082e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd81d25d-28d4-4563-9477-0a7b67d00e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Directly from data\n",
    "x = [[1,2],[3,4]]\n",
    "x_tensor = torch.tensor(x)\n",
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8181bccd-82a1-43e7-8fdb-a07b4fe74d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# From np array \n",
    "x_array = np.array(x) \n",
    "x_array_tensor = torch.tensor(x_array)\n",
    "print(x_array_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a39b51-8b23-4dfa-ad71-90219b8ab017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.7202, 0.9920],\n",
      "        [0.0935, 0.5687]])\n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# from another tensor \n",
    "x_ones = torch.ones_like(x_tensor) \n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_tensor, dtype=torch.float) # overrides the datatype of x\n",
    "print(x_rand)\n",
    "\n",
    "x_zeroes = torch.zeros_like(x_tensor)\n",
    "print(x_zeroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f60e7a-c823-4b1d-830a-1c09de362b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.3847, 0.3175, 0.9163],\n",
      "        [0.8538, 0.9047, 0.5371]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# with random or constant value \n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4aeb43-8dd5-4a84-a2df-edc7058db04a",
   "metadata": {},
   "source": [
    "In Python, adding a trailing comma after the last item in a tuple is a matter of style and doesn't affect the functionality. However, in certain contexts, it can improve code readability and make it easier to maintain, especially when modifying the tuple by adding or removing elements.\n",
    "\n",
    "The version with the trailing comma `(2, 3,)` can be considered a good practice in some coding styles or guidelines because it makes it clear that the tuple has more than one element, even if there's only one element present. This can prevent errors when adding more elements to the tuple in the future, as you won't need to remember to add a comma after the last element.\n",
    "\n",
    "In summary, while it's not strictly necessary, adding a trailing comma after the last item in a tuple can be considered a good practice for consistency and readability in Python code. However, whether or not to use it ultimately depends on the coding style guide you or your team follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5ef57-6510-4312-83fd-756d74bf4a04",
   "metadata": {},
   "source": [
    "## Tensor Attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66cc7653-0edb-4a7a-911d-caded5380b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf89c1-bdb9-445b-aefe-12ce2e10750f",
   "metadata": {},
   "source": [
    "### Basic autograds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c1ea1-f1e7-4dfc-8e82-86766481a635",
   "metadata": {},
   "source": [
    "Gradients are quite important for the optimization purpose. PyTorch provides package which can do all the computation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf79ba9c-db0c-4bf8-b16a-3c7d77753db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5194, -1.1760,  0.1726])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "068321a3-7414-4690-b26e-c97af961f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8898, -0.9223, -2.1185], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True) # by default requires_grad = false \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1038999-4d25-4182-a2bd-83672d5ba762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.8898,  1.0777, -0.1185], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2 # will create the computational graph for us \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd66dde-d223-4adf-a4c0-416f3676f204",
   "metadata": {},
   "source": [
    "Here, AddBackward can be seen as we have done the backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b82340-360c-49d9-8294-29d62a691c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.7020,  2.3231,  0.0281], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*2 \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05656eef-e31f-4eda-b255-881b0f41c71c",
   "metadata": {},
   "source": [
    "Here, MulBackward can be seen as we are doing multiplication operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260539a8-af1f-42d4-b2bb-7ff5380dc823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.3510, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = z.mean()\n",
    "print(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd10f75-158d-4d5a-8a6f-bfb7636572c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.8531,  1.4370, -0.1579])\n"
     ]
    }
   ],
   "source": [
    "# to calculate the gradient \n",
    "z.backward() # dz/dx\n",
    "print(x.grad) # x will store the gradient value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d54cce5-11b5-4dd0-a9ef-384a914e9eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.8862, 23.5957, 10.5685], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad can only be implicitly created only for scalar outputs\n",
    "x = torch.randn(3, requires_grad = True ) \n",
    "y = x + 2 \n",
    "\n",
    "z = y*y*2 \n",
    "# z.mean()\n",
    "print(z) # z is not a scalar value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc1e19a2-3789-41f1-9ffc-8e5910970a22",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# to calculate gradient \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    252\u001b[0m     (inputs,)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# to calculate gradient \n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ed0062b-c569-4a40-ae08-a803558b3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.2522e-01, 1.3739e+01, 9.1950e-03])\n"
     ]
    }
   ],
   "source": [
    "# solution is multipying it with an vector \n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float32)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbc9ef-c35a-41d7-b2ea-81f3232bfea3",
   "metadata": {},
   "source": [
    "##### Preventing Gradient History\n",
    "There are three ways to do prevent gradient history and tracking the computational history. They are: \n",
    "1. x_requires_grad_(False)\n",
    "2. x.detach()\n",
    "3. with torch.no_grad():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b09451-2516-4f28-b031-2d4d58b23fc8",
   "metadata": {},
   "source": [
    "##### 1. x_requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0da6d673-df33-41f1-947d-b6650da6837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0869, -1.6616, -0.6585], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True ) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da4a1244-ef6e-4492-8698-f0fd5884a56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhenever a function has a trailing _, this means that it will modify our variable in place. \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False) \n",
    "\"\"\"\n",
    "Whenever a function has a trailing _, this means that it will modify our variable in place. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f6e3e6-3595-4f5a-af79-a420d73ebeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0869, -1.6616, -0.6585])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c2e7cc-f2f4-4525-a991-fe569602d078",
   "metadata": {},
   "source": [
    "##### 2. x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344dcec0-c61d-4b05-be3e-e2adbbf4ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0869, -1.6616, -0.6585])\n"
     ]
    }
   ],
   "source": [
    "y = x.detach()\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40f314-22bc-4dfd-9b36-14a0582ee937",
   "metadata": {},
   "source": [
    "This will create new tensor which also does not have requires_grad> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348dfe78-46b6-43ca-ac27-aae504c8cca6",
   "metadata": {},
   "source": [
    "##### 3. with toch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2994bf0-405f-43c7-b10a-aa427066dc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9131, 0.3384, 1.3415])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y =  x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb416ea-0a71-4c52-823b-a8a615947297",
   "metadata": {},
   "source": [
    "This also does not have the gradient attribute. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fdf53-6cf8-4500-b07d-75590bc6ccff",
   "metadata": {},
   "source": [
    "Now,\n",
    "In PyTorch, when you call the backward() function on a tensor that is part of a computation graph, it computes the gradients of some scalar value (usually a loss) with respect to that tensor, using automatic differentiation techniques like backpropagation.\n",
    "\n",
    "When you perform backpropagation, PyTorch accumulates the gradients of the parameters (usually weights) of your model in their respective .grad attributes. These gradients are not overwritten on subsequent calls to backward(), but rather accumulated. This behavior is useful, for example, when you have multiple losses in your model and you want to accumulate gradients from each loss before updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdadf808-5876-4cbb-9ba6-a274baa41e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "weight = torch.ones(4, requires_grad = True) \n",
    "\n",
    "for epoch in range(1):\n",
    "    model_output = (weight*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e894b25-ac1a-47ea-88c8-cb3c6e04f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.ones(4, requires_grad = True) \n",
    "\n",
    "for epoch in range(2):\n",
    "    model_output = (weight*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f801eb03-cff0-4df8-9f0b-99c2d75d9886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.ones(4, requires_grad = True) \n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weight*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7d19c-d030-4ee4-8d82-1a8ea7f892e6",
   "metadata": {},
   "source": [
    "All the values are summed up and our weights are clearly incorrect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93341023-8a93-498b-9d6a-cc44cf781ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Before we do next iteration and optimization step , we must empty the gradient. \n",
    "weight = torch.ones(4, requires_grad = True) \n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weight*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weight.grad)\n",
    "    weight.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bfe09-2594-4ecf-a1d0-fe8ecef7b45d",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61a98bdb-524b-4d71-8f1f-7314dcd3d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c221013a-6658-444e-8619-74d4c61e9a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass and compute the loss\n",
    "y_hat = w*x \n",
    "loss = (y_hat - y )**2\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "add893d4-e6c2-4cfc-b943-9457ebb5c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "739abd85-eb27-45a8-b29b-d28215a5f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights \n",
    "### next forward and backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f82d0f-00ac-4ddf-8b53-0ad685b8ce08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
