{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What is PyTorch ? \nPyTorch is an open source machine learning and deep leaning framework. ","metadata":{}},{"cell_type":"markdown","source":"#### What can PyTorch be used for?\nPyTorch allows you to manipulate and process data and write machine learning algorithms using Python code.","metadata":{}},{"cell_type":"markdown","source":"####  Why use PyTorch?\nMachine learning researchers love using PyTorch. PyTorch is the most used deep learning framework on Papers With Code, a website for tracking machine learning research papers and the code repositories attached with them.\n\nPyTorch also helps take care of many things such as GPU acceleration (making your code run faster) behind the scenes.\n\nSo you can focus on manipulating data and writing algorithms and PyTorch will make sure it runs fast.\n\nAnd if companies such as Tesla and Meta (Facebook) use it to build models they deploy to power hundreds of applications, drive thousands of cars and deliver content to billions of people, it's clearly capable on the development front too.","metadata":{}},{"cell_type":"markdown","source":"#### Importing PyTorch ","metadata":{}},{"cell_type":"code","source":"import torch\n\n# check the version \ntorch.__version__","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:25.439679Z","iopub.execute_input":"2024-05-18T06:15:25.440019Z","iopub.status.idle":"2024-05-18T06:15:29.501840Z","shell.execute_reply.started":"2024-05-18T06:15:25.439990Z","shell.execute_reply":"2024-05-18T06:15:29.500850Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'2.1.2'"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Introduction to Tensor \nTensors are n-dimensional array. ","metadata":{}},{"cell_type":"markdown","source":"#### Creating Tensor ","metadata":{}},{"cell_type":"code","source":"# scalar \n# A scalar is a single number and in tensor-speak it's a zero dimension tensor.\nscalar = torch.tensor(7)\nprint(scalar)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:32.673323Z","iopub.execute_input":"2024-05-18T06:15:32.674246Z","iopub.status.idle":"2024-05-18T06:15:32.718263Z","shell.execute_reply.started":"2024-05-18T06:15:32.674214Z","shell.execute_reply":"2024-05-18T06:15:32.717365Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"tensor(7)\n","output_type":"stream"}]},{"cell_type":"code","source":"scalar.ndim","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:33.093395Z","iopub.execute_input":"2024-05-18T06:15:33.093763Z","iopub.status.idle":"2024-05-18T06:15:33.099502Z","shell.execute_reply.started":"2024-05-18T06:15:33.093735Z","shell.execute_reply":"2024-05-18T06:15:33.098551Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# now if I want to retrieve the number from tensor \n# Get the Python number within a tensor (only works with one-element tensors)\nscalar.item()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:33.363545Z","iopub.execute_input":"2024-05-18T06:15:33.363837Z","iopub.status.idle":"2024-05-18T06:15:33.369688Z","shell.execute_reply.started":"2024-05-18T06:15:33.363813Z","shell.execute_reply":"2024-05-18T06:15:33.368638Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"7"},"metadata":{}}]},{"cell_type":"code","source":"# vector \n# A vector is a single dimension tensor but can contain many numbers.\nvector = torch.tensor([1,3,4])\nprint(vector)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:33.650909Z","iopub.execute_input":"2024-05-18T06:15:33.651256Z","iopub.status.idle":"2024-05-18T06:15:33.660079Z","shell.execute_reply.started":"2024-05-18T06:15:33.651228Z","shell.execute_reply":"2024-05-18T06:15:33.658977Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tensor([1, 3, 4])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**How does the shape affects the dimension of the tensor ?** \nA tensor can have more than two dimensions. The dimensionality (or rank) of a tensor is the number of indices required to uniquely specify an element of the tensor.\n\nWhat does this means ? \nLet's say we have an array of a= [1,2,3]. So now if we are trying to access the element then\na[0] = 1 \na[1] = 2\na[2] = 3 \nHere, we can access the element with single indices. ","metadata":{}},{"cell_type":"code","source":"vector.ndim","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:34.153788Z","iopub.execute_input":"2024-05-18T06:15:34.154132Z","iopub.status.idle":"2024-05-18T06:15:34.159832Z","shell.execute_reply.started":"2024-05-18T06:15:34.154105Z","shell.execute_reply":"2024-05-18T06:15:34.158912Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"# check the shape of the vector \nvector.shape ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:34.395982Z","iopub.execute_input":"2024-05-18T06:15:34.396501Z","iopub.status.idle":"2024-05-18T06:15:34.402242Z","shell.execute_reply.started":"2024-05-18T06:15:34.396471Z","shell.execute_reply":"2024-05-18T06:15:34.401329Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([3])"},"metadata":{}}]},{"cell_type":"markdown","source":"**Fun Fact: Shape of a Vector**\n\n- One-Dimensional Vector:\n\nWhen we talk about a vector such as [1,3,4], it is commonly considered a one-dimensional array.\nIn this context, its shape is simply (3,), indicating it has 3 elements in one dimension.\n\n- Matrix Interpretation:\n\nIf we interpret [1,3,4] as a row vector in the context of a matrix, then it can indeed be viewed as a matrix with 1 row and 3 columns.\nIn this case, the shape would be (1,3).\n\n> Detailed Examples\n- As a One-Dimensional Vector:\n\nConsider [1,3,4] as a 1D array.\nShape: (3,), indicating a single dimension with 3 elements.\n\n- As a Row Vector in a Matrix:\n\nInterpreting [1,3,4] as a row vector in matrix form:\nShape: (1,3), indicating 1 row and 3 columns.\n\n- As a Column Vector:\n\nIf [1,3,4] were instead considered a column vector.\nShape: (3,1), indicating 3 rows and 1 column.","metadata":{}},{"cell_type":"markdown","source":"vector has a shape of [3]. This is because of the two elements we placed inside the square brackets ([1,3,4]).","metadata":{}},{"cell_type":"code","source":"# Matrix \nmatrix = torch.tensor([[1,2],\n                       [4,5]])\nmatrix ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:35.045008Z","iopub.execute_input":"2024-05-18T06:15:35.045366Z","iopub.status.idle":"2024-05-18T06:15:35.052716Z","shell.execute_reply.started":"2024-05-18T06:15:35.045338Z","shell.execute_reply":"2024-05-18T06:15:35.051701Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 2],\n        [4, 5]])"},"metadata":{}}]},{"cell_type":"code","source":"matrix.ndim","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:35.272881Z","iopub.execute_input":"2024-05-18T06:15:35.273449Z","iopub.status.idle":"2024-05-18T06:15:35.278910Z","shell.execute_reply.started":"2024-05-18T06:15:35.273419Z","shell.execute_reply":"2024-05-18T06:15:35.278040Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"print(matrix[0][0]) \nprint(matrix[1][0])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:35.478700Z","iopub.execute_input":"2024-05-18T06:15:35.479005Z","iopub.status.idle":"2024-05-18T06:15:35.485098Z","shell.execute_reply.started":"2024-05-18T06:15:35.478981Z","shell.execute_reply":"2024-05-18T06:15:35.484050Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tensor(1)\ntensor(4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here we need two indices to access the element. Thus the dimension of the tensor is 2. ","metadata":{}},{"cell_type":"markdown","source":"The matrix having the shape of (2,2) is considered as 2 dimensional as it has two directions x and y. ","metadata":{}},{"cell_type":"code","source":"matrix.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:36.080224Z","iopub.execute_input":"2024-05-18T06:15:36.080574Z","iopub.status.idle":"2024-05-18T06:15:36.086334Z","shell.execute_reply.started":"2024-05-18T06:15:36.080547Z","shell.execute_reply":"2024-05-18T06:15:36.085482Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 2])"},"metadata":{}}]},{"cell_type":"code","source":"# Tensor \ntensor = torch.tensor([[[1,2,3],\n                        [3,3,3],\n                        [6,6,8]]])\ntensor ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:36.282112Z","iopub.execute_input":"2024-05-18T06:15:36.282453Z","iopub.status.idle":"2024-05-18T06:15:36.289820Z","shell.execute_reply.started":"2024-05-18T06:15:36.282426Z","shell.execute_reply":"2024-05-18T06:15:36.288768Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([[[1, 2, 3],\n         [3, 3, 3],\n         [6, 6, 8]]])"},"metadata":{}}]},{"cell_type":"code","source":"tensor.shape ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:36.503955Z","iopub.execute_input":"2024-05-18T06:15:36.504237Z","iopub.status.idle":"2024-05-18T06:15:36.509848Z","shell.execute_reply.started":"2024-05-18T06:15:36.504212Z","shell.execute_reply":"2024-05-18T06:15:36.508941Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 3, 3])"},"metadata":{}}]},{"cell_type":"code","source":"tensor[0][2][2]\n# [0]: Accesses the first (and only) matrix in the tensor.\n# [2]: Accesses the third row of the matrix.\n# [2]: Accesses the third element in that row, which is 8.","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:36.692975Z","iopub.execute_input":"2024-05-18T06:15:36.693332Z","iopub.status.idle":"2024-05-18T06:15:36.700571Z","shell.execute_reply.started":"2024-05-18T06:15:36.693294Z","shell.execute_reply":"2024-05-18T06:15:36.699463Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor(8)"},"metadata":{}}]},{"cell_type":"code","source":"tensor.ndim","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:36.894775Z","iopub.execute_input":"2024-05-18T06:15:36.895146Z","iopub.status.idle":"2024-05-18T06:15:36.901369Z","shell.execute_reply.started":"2024-05-18T06:15:36.895115Z","shell.execute_reply":"2024-05-18T06:15:36.900337Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"markdown","source":"The dimensions go outer to inner.\n\nThat means there's 1 dimension of 3 by 3.","metadata":{}},{"cell_type":"code","source":"# when to use [1][][]\ntensor_list = [\n    torch.tensor([[1, 2, 3],\n                  [3, 3, 3],\n                  [6, 6, 8]]),\n    torch.tensor([[9, 10, 11],\n                  [12, 13, 14],\n                  [15, 16, 17]])\n]\n\nelement = tensor_list[1][2][2]\nprint(element) ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:37.294778Z","iopub.execute_input":"2024-05-18T06:15:37.295114Z","iopub.status.idle":"2024-05-18T06:15:37.301984Z","shell.execute_reply.started":"2024-05-18T06:15:37.295085Z","shell.execute_reply":"2024-05-18T06:15:37.301058Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tensor(17)\n","output_type":"stream"}]},{"cell_type":"code","source":"tensor = torch.tensor([[1,2],\n                      [3,4],\n                      [6,7]])\ntensor ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:37.502885Z","iopub.execute_input":"2024-05-18T06:15:37.503165Z","iopub.status.idle":"2024-05-18T06:15:37.510139Z","shell.execute_reply.started":"2024-05-18T06:15:37.503134Z","shell.execute_reply":"2024-05-18T06:15:37.509293Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 2],\n        [3, 4],\n        [6, 7]])"},"metadata":{}}]},{"cell_type":"code","source":"tensor.shape ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:37.699194Z","iopub.execute_input":"2024-05-18T06:15:37.699527Z","iopub.status.idle":"2024-05-18T06:15:37.705255Z","shell.execute_reply.started":"2024-05-18T06:15:37.699503Z","shell.execute_reply":"2024-05-18T06:15:37.704422Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 2])"},"metadata":{}}]},{"cell_type":"code","source":"# random tensor \nrandom_tensor = torch.randn(3,4)\nrandom_tensor , random_tensor.dtype","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:37.907121Z","iopub.execute_input":"2024-05-18T06:15:37.907418Z","iopub.status.idle":"2024-05-18T06:15:37.992924Z","shell.execute_reply.started":"2024-05-18T06:15:37.907392Z","shell.execute_reply":"2024-05-18T06:15:37.992059Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 0.0079, -0.1512, -0.2071, -0.3022],\n         [-0.0874,  0.7005,  0.7586,  1.3575],\n         [-0.6072,  0.5573,  2.7717, -0.5877]]),\n torch.float32)"},"metadata":{}}]},{"cell_type":"code","source":"# Create a random tensor of size (224, 224, 3) = img size \nrandom_image_size_tensor = torch.rand(size=(224, 224, 3))\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:38.118772Z","iopub.execute_input":"2024-05-18T06:15:38.119393Z","iopub.status.idle":"2024-05-18T06:15:38.127532Z","shell.execute_reply.started":"2024-05-18T06:15:38.119365Z","shell.execute_reply":"2024-05-18T06:15:38.126639Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(torch.Size([224, 224, 3]), 3)"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Zeros and ones ","metadata":{}},{"cell_type":"code","source":"zeros_tensor = torch.zeros(3,4)\nzeros_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:38.532535Z","iopub.execute_input":"2024-05-18T06:15:38.533229Z","iopub.status.idle":"2024-05-18T06:15:38.540257Z","shell.execute_reply.started":"2024-05-18T06:15:38.533197Z","shell.execute_reply":"2024-05-18T06:15:38.539300Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"ones_tensor = torch.ones(3,4)\nones_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:38.728571Z","iopub.execute_input":"2024-05-18T06:15:38.728848Z","iopub.status.idle":"2024-05-18T06:15:38.737154Z","shell.execute_reply.started":"2024-05-18T06:15:38.728825Z","shell.execute_reply":"2024-05-18T06:15:38.736195Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Tensor DataType \nThere are many different tensor datatypes available in PyTorch.\n\nSome are specific for CPU and some are better for GPU.\n\nGetting to know which is which can take some time.\n\nGenerally if you see torch.cuda anywhere, the tensor is being used for GPU (since Nvidia GPUs use a computing toolkit called CUDA).\n\nThe most common type (and generally the default) is torch.float32 or torch.float.\n\nThis is referred to as \"32-bit floating point\".\n\nBut there's also 16-bit floating point (torch.float16 or torch.half) and 64-bit floating point (torch.float64 or torch.double).\n\nAnd to confuse things even more there's also 8-bit, 16-bit, 32-bit and 64-bit integers.","metadata":{}},{"cell_type":"code","source":"float32_tensor = torch.tensor([3.0, 6.0 ,9.0],\n                              requires_grad = False,\n                              device = None,\n                              dtype = None)\n\nfloat32_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:39.166966Z","iopub.execute_input":"2024-05-18T06:15:39.167291Z","iopub.status.idle":"2024-05-18T06:15:39.174427Z","shell.execute_reply.started":"2024-05-18T06:15:39.167252Z","shell.execute_reply":"2024-05-18T06:15:39.173614Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([3., 6., 9.])"},"metadata":{}}]},{"cell_type":"code","source":"float32_tensor.shape, float32_tensor.dtype, float32_tensor.device","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:39.389336Z","iopub.execute_input":"2024-05-18T06:15:39.389639Z","iopub.status.idle":"2024-05-18T06:15:39.395246Z","shell.execute_reply.started":"2024-05-18T06:15:39.389613Z","shell.execute_reply":"2024-05-18T06:15:39.394400Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(torch.Size([3]), torch.float32, device(type='cpu'))"},"metadata":{}}]},{"cell_type":"markdown","source":"Aside from shape issues (tensor shapes don't match up), two of the other most common issues you'll come across in PyTorch are datatype and device issues.\n\nFor example, one of tensors is torch.float32 and the other is torch.float16 (PyTorch often likes tensors to be the same format).\n\nOr one of your tensors is on the CPU and the other is on the GPU (PyTorch likes calculations between tensors to be on the same device).","metadata":{}},{"cell_type":"markdown","source":"#### Tensor Multiplication ","metadata":{}},{"cell_type":"code","source":"tensor = torch.tensor([1,2,3])\ntensor.shape ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:39.994315Z","iopub.execute_input":"2024-05-18T06:15:39.994626Z","iopub.status.idle":"2024-05-18T06:15:40.000884Z","shell.execute_reply.started":"2024-05-18T06:15:39.994602Z","shell.execute_reply":"2024-05-18T06:15:39.999982Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"torch.Size([3])"},"metadata":{}}]},{"cell_type":"code","source":"# Element-wise matrix multiplication\ntensor * tensor ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:40.196112Z","iopub.execute_input":"2024-05-18T06:15:40.196863Z","iopub.status.idle":"2024-05-18T06:15:40.205169Z","shell.execute_reply.started":"2024-05-18T06:15:40.196830Z","shell.execute_reply":"2024-05-18T06:15:40.203830Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([1, 4, 9])"},"metadata":{}}]},{"cell_type":"code","source":"# Matrix multiplication\ntorch.matmul(tensor, tensor)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:40.412301Z","iopub.execute_input":"2024-05-18T06:15:40.412578Z","iopub.status.idle":"2024-05-18T06:15:40.421135Z","shell.execute_reply.started":"2024-05-18T06:15:40.412553Z","shell.execute_reply":"2024-05-18T06:15:40.420327Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor(14)"},"metadata":{}}]},{"cell_type":"code","source":"# Can also use the \"@\" symbol for matrix multiplication, though not recommended\ntensor @ tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:40.628661Z","iopub.execute_input":"2024-05-18T06:15:40.629029Z","iopub.status.idle":"2024-05-18T06:15:40.636021Z","shell.execute_reply.started":"2024-05-18T06:15:40.628997Z","shell.execute_reply":"2024-05-18T06:15:40.634989Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"tensor(14)"},"metadata":{}}]},{"cell_type":"markdown","source":"The in-built torch.matmul() method is faster","metadata":{}},{"cell_type":"code","source":"tensor = torch.tensor([1, 2, 3])\ntensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:41.063534Z","iopub.execute_input":"2024-05-18T06:15:41.063859Z","iopub.status.idle":"2024-05-18T06:15:41.070020Z","shell.execute_reply.started":"2024-05-18T06:15:41.063836Z","shell.execute_reply":"2024-05-18T06:15:41.069177Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"torch.Size([3])"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# Matrix multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:41.279204Z","iopub.execute_input":"2024-05-18T06:15:41.279521Z","iopub.status.idle":"2024-05-18T06:15:41.292623Z","shell.execute_reply.started":"2024-05-18T06:15:41.279496Z","shell.execute_reply":"2024-05-18T06:15:41.291682Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"CPU times: user 1.63 ms, sys: 917 µs, total: 2.55 ms\nWall time: 5.84 ms\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor(14)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\ntorch.matmul(tensor, tensor)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:41.491520Z","iopub.execute_input":"2024-05-18T06:15:41.491800Z","iopub.status.idle":"2024-05-18T06:15:41.499219Z","shell.execute_reply.started":"2024-05-18T06:15:41.491778Z","shell.execute_reply":"2024-05-18T06:15:41.498316Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"CPU times: user 336 µs, sys: 84 µs, total: 420 µs\nWall time: 395 µs\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"tensor(14)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Running tensors on GPU (and making faster computations)","metadata":{}},{"cell_type":"markdown","source":"Deep learning algorithms require a lot of numerical operations.\n\nAnd by default these operations are often done on a CPU (computer processing unit).\n\nHowever, there's another common piece of hardware called a GPU (graphics processing unit), which is often much faster at performing the specific types of operations neural networks need (matrix multiplications) than CPUs.\n\nYour computer might have one.\n\nIf so, you should look to use it whenever you can to train neural networks because chances are it'll speed up the training time dramatically.\n\nThere are a few ways to first get access to a GPU and secondly get PyTorch to use the GPU.\n\nNote: When I reference \"GPU\" throughout this course, I'm referencing a Nvidia GPU with CUDA enabled (CUDA is a computing platform and API that helps allow GPUs be used for general purpose computing & not just graphics) unless otherwise specified.","metadata":{}},{"cell_type":"markdown","source":"#### 1. Getting a GPU\nTo check if you've got access to a Nvidia GPU, you can run !nvidia-smi where the ! (also called bang) means \"run this on the command line\".","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:15:46.740635Z","iopub.execute_input":"2024-05-18T06:15:46.740977Z","iopub.status.idle":"2024-05-18T06:15:47.790734Z","shell.execute_reply.started":"2024-05-18T06:15:46.740948Z","shell.execute_reply":"2024-05-18T06:15:47.789363Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Sat May 18 06:15:47 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 2. Getting PyTorch to run on the GPU\n","metadata":{}},{"cell_type":"code","source":"# Check for GPU\nimport torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:18:16.718994Z","iopub.execute_input":"2024-05-18T06:18:16.719414Z","iopub.status.idle":"2024-05-18T06:18:16.782514Z","shell.execute_reply.started":"2024-05-18T06:18:16.719382Z","shell.execute_reply":"2024-05-18T06:18:16.781474Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"If the above outputs True, PyTorch can see and use the GPU, if it outputs False, it can't see the GPU and in that case, you'll have to go back through the installation steps.\n\nNow, let's say you wanted to setup your code so it ran on CPU or the GPU if it was available.\n\nThat way, if you or someone decides to run your code, it'll work regardless of the computing device they're using.\n\nLet's create a device variable to store what kind of device is available.","metadata":{}},{"cell_type":"code","source":"# Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:19:02.218683Z","iopub.execute_input":"2024-05-18T06:19:02.219039Z","iopub.status.idle":"2024-05-18T06:19:02.225312Z","shell.execute_reply.started":"2024-05-18T06:19:02.219012Z","shell.execute_reply":"2024-05-18T06:19:02.224460Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"If the above output \"cuda\" it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output \"cpu\", our PyTorch code will stick with the CPU.","metadata":{}},{"cell_type":"code","source":"# Count number of devices\ntorch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:19:46.087424Z","iopub.execute_input":"2024-05-18T06:19:46.088136Z","iopub.status.idle":"2024-05-18T06:19:46.117115Z","shell.execute_reply.started":"2024-05-18T06:19:46.088103Z","shell.execute_reply":"2024-05-18T06:19:46.116140Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}